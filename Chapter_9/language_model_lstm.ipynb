{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0gTyfL_Huko"
   },
   "source": [
    "# LSTM Based Language Model\n",
    "A language model looks at the context to generate next set of words. This context is also called as a sliding window which moves across the input sentence from left to right(right to left for language which are written from right to left). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/blob/master/Chapter_9/language_model_lstm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laf8nuPYIRQO"
   },
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqviC-HVpmm2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "GM1hw8f4p67G",
    "outputId": "d859baaa-a5d6-4885-b0d7-6cefd1c79cb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version=2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version={}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZ3sFTVvIhwD"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nr9VOpsop81u"
   },
   "outputs": [],
   "source": [
    "# https://www.gutenberg.org/ebooks/2600\n",
    "datafile_path = r'warpeace_2600-0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "g_6iVpkqqEAN",
    "outputId": "495012d8-d458-4895-c4ce-9583a743fa55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book contains a total of 3293673 characters\n"
     ]
    }
   ],
   "source": [
    "# Load the text file\n",
    "text = open(datafile_path, 'rb').read().decode(encoding='utf-8')\n",
    "print ('Book contains a total of {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "WUcBctq8qFDq",
    "outputId": "58bc914d-7879-4681-82be-d2ed2c85ded8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "BOOK ONE: 1805\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "CHAPTER I\r\n",
      "\r\n",
      "“Well, Prince, so Genoa and Lucca are now just family estates of the\r\n",
      "Buonapartes. But I warn you, if you don’t tell me that this means war,\r\n",
      "if you still try to defend the infamies and horrors perpetrated by that\r\n",
      "Antichrist—I really believe he is Antichrist—I will have nothing\r\n",
      "more to do with you and you are no longer my friend, no longer my\r\n",
      "‘faithful slave,’ as you call yourself! But how do you do? I see I\r\n",
      "have frightened you—sit down and tell me al\n"
     ]
    }
   ],
   "source": [
    "idx = 8091\n",
    "print(text[idx:idx+500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NK5VucWVqNiG"
   },
   "outputs": [],
   "source": [
    "# We remove first 8k characters to remove \n",
    "# details related to project gutenberg\n",
    "text = text [8091:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "BgqxMDT1qN1j",
    "outputId": "b55ec612-7891-4871-c5ba-3150b69c1df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7K7kUR6Iup6"
   },
   "source": [
    "## Prepare Dataset\n",
    "+ Dictionary of character to index mapping\n",
    "+ Inverse mapping of index to character mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-1s-z3RqOG7"
   },
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "14v7MGryqOV2",
    "outputId": "54986d23-d4d3-495e-9f18-3ca043c2c024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  '!' :   3,\n",
      "  '$' :   4,\n",
      "  '%' :   5,\n",
      "  '(' :   6,\n",
      "  ')' :   7,\n",
      "  '*' :   8,\n",
      "  ',' :   9,\n",
      "  '-' :  10,\n",
      "  '.' :  11,\n",
      "  '/' :  12,\n",
      "  '0' :  13,\n",
      "  '1' :  14,\n",
      "  '2' :  15,\n",
      "  '3' :  16,\n",
      "  '4' :  17,\n",
      "  '5' :  18,\n",
      "  '6' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6kEpFB1I9NY"
   },
   "source": [
    "### Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "Dp_GTAFiqVq1",
    "outputId": "42f0c663-41da-4c1b-d0ed-33a0c35e243b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n“Well, Prince, so G' ---- char-2-int ----  [  0 106  50  58  65  65   9   2  43  71  62  67  56  58   9   2  72  68\n",
      "   2  34]\n"
     ]
    }
   ],
   "source": [
    "print ('{} ---- char-2-int ----  {}'.format(repr(text[40:60]), text_as_int[40:60]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Emy0LrJtJFbw"
   },
   "source": [
    "### Prepare Batch of Training Samples\n",
    "+ Sequence length limit to 100\n",
    "+ Use ``tf.data`` API to prepare batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "0XdE0xYcqWJC",
    "outputId": "5794a006-cb87-4bf8-8b3d-06c8f638f25b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\n",
      "\n",
      "B\n",
      "O\n",
      "O\n",
      "K\n",
      " \n",
      "O\n",
      "N\n",
      "E\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "ws-2k5p2qWWG",
    "outputId": "9906f799-fb46-4533-d5c2-2d51fde64e88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\r\\nBOOK ONE: 1805\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCHAPTER I\\r\\n\\r\\n“Well, Prince, so Genoa and Lucca are now just family estate'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'s of the\\r\\nBuonapartes. But I warn you, if you don’t tell me that this means war,\\r\\nif you still try to'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "' defend the infamies and horrors perpetrated by that\\r\\nAntichrist—I really believe he is Antichrist—I '\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'will have nothing\\r\\nmore to do with you and you are no longer my friend, no longer my\\r\\n‘faithful slave'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "',’ as you call yourself! But how do you do? I see I\\r\\nhave frightened you—sit down and tell me all the'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "' news.”\\r\\n\\r\\nIt was in July, 1805, and the speaker was the well-known Anna Pávlovna\\r\\nSchérer, maid of h'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'onor and favorite of the Empress Márya Fëdorovna.\\r\\nWith these words she greeted Prince Vasíli Kurágin'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "', a man of high\\r\\nrank and importance, who was the first to arrive at her reception. Anna\\r\\nPávlovna ha'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'d had a cough for some days. She was, as she said, suffering\\r\\nfrom la grippe; grippe being then a new'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "' word in St. Petersburg, used\\r\\nonly by the elite.\\r\\n\\r\\nAll her invitations without exception, written i'\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(10):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "    print(\"-\"*110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfJ8lzfSJuBU"
   },
   "source": [
    "### Prepare Input->Target samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-7b_73PqWji"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    \"\"\"\n",
    "    Utility which takes a chunk of input text and target as one position shifted form of input chunk.\n",
    "    Parameters:\n",
    "        chunk: input list of words\n",
    "    Returns:\n",
    "        Tuple-> input_text(i.e. chunk minus last word),target_text(input chunk minus the first word)\n",
    "    \"\"\"\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "ZNpd4pilqWw2",
    "outputId": "f2627e9a-20d9-451b-ccfe-6643635e8cba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '\\r\\nBOOK ONE: 1805\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCHAPTER I\\r\\n\\r\\n“Well, Prince, so Genoa and Lucca are now just family estat'\n",
      "Target data: '\\nBOOK ONE: 1805\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCHAPTER I\\r\\n\\r\\n“Well, Prince, so Genoa and Lucca are now just family estate'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MW75dGqMqlAj"
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 128\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "duuMnJEDqlNX",
    "outputId": "0d3023b6-e546-4ad1-c937-2eb9a3ea446c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape=<BatchDataset shapes: ((128, 100), (128, 100)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(\"Dataset Shape={}\".format(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qL12LqyWJ0tn"
   },
   "source": [
    "## Prepare Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_sIzibsqlof"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    \"\"\"\n",
    "    Utility to create a model object.\n",
    "    Parameters:\n",
    "        vocab_size: number of unique characters\n",
    "        embedding_dim: size of embedding vector. This typically in powers of 2, i.e. 64, 128, 256 and so on\n",
    "        rnn_units: number of LSTM units to be used\n",
    "        batch_size: batch size for training the model\n",
    "    Returns:\n",
    "        tf.keras model object\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipJBSRk_KEGK"
   },
   "source": [
    "### Define the Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BeShRs8qwCF"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-LCpWVEqwU0"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "iujYNi4cqv5N",
    "outputId": "18ea1966-8377-42da-ae51-47322c7e334f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 256)          27648     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (128, None, 1024)         5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 108)          110700    \n",
      "=================================================================\n",
      "Total params: 5,385,324\n",
      "Trainable params: 5,385,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAUK1SDWql6N"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xM5dHsLwrx-S"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAMJXmvYKSGh"
   },
   "source": [
    "### Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BpfQbQXqmec"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = r'data/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "m6-WTUO9q7is",
    "outputId": "c6f76aa7-947d-4229-94ad-668f2dcd7efe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "254/254 [==============================] - 35s 138ms/step - loss: 2.5342\n",
      "Epoch 2/25\n",
      "254/254 [==============================] - 37s 146ms/step - loss: 1.9065\n",
      "Epoch 3/25\n",
      "254/254 [==============================] - 36s 142ms/step - loss: 1.6550\n",
      "Epoch 4/25\n",
      "254/254 [==============================] - 37s 145ms/step - loss: 1.5129\n",
      "Epoch 5/25\n",
      "254/254 [==============================] - 36s 144ms/step - loss: 1.4262\n",
      "Epoch 6/25\n",
      "254/254 [==============================] - 37s 145ms/step - loss: 1.3673\n",
      "Epoch 7/25\n",
      "254/254 [==============================] - 37s 145ms/step - loss: 1.3241\n",
      "Epoch 8/25\n",
      "254/254 [==============================] - 37s 145ms/step - loss: 1.2908\n",
      "Epoch 9/25\n",
      "254/254 [==============================] - 37s 145ms/step - loss: 1.2639\n",
      "Epoch 10/25\n",
      "254/254 [==============================] - 37s 145ms/step - loss: 1.2417\n",
      "Epoch 11/25\n",
      "254/254 [==============================] - 37s 144ms/step - loss: 1.2230\n",
      "Epoch 12/25\n",
      "254/254 [==============================] - 37s 144ms/step - loss: 1.2068\n",
      "Epoch 13/25\n",
      "254/254 [==============================] - 37s 145ms/step - loss: 1.1921\n",
      "Epoch 14/25\n",
      "254/254 [==============================] - 37s 145ms/step - loss: 1.1787\n",
      "Epoch 15/25\n",
      "254/254 [==============================] - 37s 145ms/step - loss: 1.1669\n",
      "Epoch 16/25\n",
      "254/254 [==============================] - 37s 145ms/step - loss: 1.1558\n",
      "Epoch 17/25\n",
      "254/254 [==============================] - 37s 145ms/step - loss: 1.1452\n",
      "Epoch 18/25\n",
      "254/254 [==============================] - 37s 144ms/step - loss: 1.1357\n",
      "Epoch 19/25\n",
      "254/254 [==============================] - 37s 144ms/step - loss: 1.1263\n",
      "Epoch 20/25\n",
      "254/254 [==============================] - 37s 145ms/step - loss: 1.1170\n",
      "Epoch 21/25\n",
      "254/254 [==============================] - 37s 145ms/step - loss: 1.1088\n",
      "Epoch 22/25\n",
      "254/254 [==============================] - 37s 145ms/step - loss: 1.1000\n",
      "Epoch 23/25\n",
      "254/254 [==============================] - 37s 144ms/step - loss: 1.0923\n",
      "Epoch 24/25\n",
      "254/254 [==============================] - 37s 145ms/step - loss: 1.0846\n",
      "Epoch 25/25\n",
      "254/254 [==============================] - 37s 144ms/step - loss: 1.0772\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IH2kBWPjKZ_p"
   },
   "source": [
    "## Generate Fake Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5O5OelGKokv"
   },
   "source": [
    "### Load Latest Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "710QCwSJq78O",
    "outputId": "2ba9170a-0820-49f5-8ad8-8dafbff3144b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'data/training_checkpoints/ckpt_25'"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch the latest checkpoint from the model directory\n",
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C33Lo5A4q7yQ"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "VM5cCjyeq7ZP",
    "outputId": "4393c680-0bce-41c7-8e66-444709dbbfcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            27648     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 108)            110700    \n",
      "=================================================================\n",
      "Total params: 5,385,324\n",
      "Trainable params: 5,385,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_J0tN6RK43x"
   },
   "source": [
    "### Utility Function to Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zge9K2AIO3TF"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, mode='greedy', context_string='Hello', num_generate=1000, \n",
    "                  temperature=1.0):\n",
    "    \"\"\"\n",
    "    Utility to generate text given a trained model and context\n",
    "    Parameters:\n",
    "        model: tf.keras object trained on a sufficiently sized corpus\n",
    "        mode: decoding mode. Default is greedy. Other mode is\n",
    "              sampling (set temperature)\n",
    "        context_string: input string which acts as context for the model\n",
    "        num_generate: number of characters to be generated\n",
    "        temperature: parameter to control randomness of outputs\n",
    "    Returns:\n",
    "        string : context_string+text_generated\n",
    "    \"\"\"\n",
    "\n",
    "    # vectorizing: convert context string into string indices\n",
    "    input_eval = [char2idx[s] for s in context_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # String for generated characters\n",
    "    text_generated = []\n",
    "    beam_input_predictions = []\n",
    "    model.reset_states()\n",
    "    # Loop till required number of characters are generated\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        if mode == 'greedy':\n",
    "          predicted_id = np.argmax(predictions[0])\n",
    "          \n",
    "        elif mode == 'sampling':\n",
    "          # temperature helps control the character returned by the model.\n",
    "          predictions = predictions / temperature\n",
    "          # Sampling over a categorical distribution\n",
    "          predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # predicted character acts as input for next step\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    return (context_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSviXqdnK-gW"
   },
   "source": [
    "### Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "JP3bh8OorG9c",
    "outputId": "8be9af34-0da9-4ff3-e28f-580282c49164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was in July, 1805-\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "CHAPTER XII\r\n",
      "\r\n",
      "The former conditions of\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, mode= 'greedy', context_string=u\"It was in July, 1805\",num_generate=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbFJp-h8LF6E"
   },
   "source": [
    "### Sampled Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "feWUSIl0dMlu",
    "outputId": "f8130721-a6de-47b1-ccc4-148892239921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was in July, 1805,\r\n",
      "\r\n",
      "“Yes, I say, sir, and so it is the same thing!” said the countess, with a smile of the same tim\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, mode= 'sampling', context_string=u\"It was in July, 1805\",num_generate=100,temperature=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "KeEEU42VcNkH",
    "outputId": "572f5eb9-1ef5-4ecc-dd17-e00769cfd57c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was in July, 1805, and the country former\r\n",
      "adjutants were no longer than as it was done herself with his stories and \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, mode= 'sampling', context_string=u\"It was in July, 1805\",num_generate=100,temperature=0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "iOI_Yh5-WS9t",
    "outputId": "863246e9-6ec8-4ec4-a242-191aa8ac64fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was in July, 1805, I spoke to them,\r\n",
      "and Bonaparte was foreshed the effect one\r\n",
      "another; intelligent, or by asking to\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, mode= 'sampling', context_string=u\"It was in July, 1805\",num_generate=100,temperature=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oci-UXlVgzfI"
   },
   "outputs": [],
   "source": [
    "def dummy(ctr,max,p_list,s):\n",
    "  print(ctr,p_list)\n",
    "  print(\"*********\")\n",
    "  if ctr == max:\n",
    "    return -1\n",
    "  rt = []\n",
    "  for i in range(s):\n",
    "    rt.append([p_list[i],dummy(ctr+1,max,p_list[1:],s)])\n",
    "  return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojSMNqBCEkRK"
   },
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "max = 3\n",
    "p_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "x = dummy(ctr,max,p_list,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDPPRnG-PRAn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "language_model_lstm.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
