{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0gTyfL_Huko"
   },
   "source": [
    "# LSTM Based Language Model - Part II\n",
    "A language model looks at the context to generate next set of words. This context is also called as a sliding window which moves across the input sentence from left to right(right to left for language which are written from right to left). \n",
    "\n",
    "This is the second notebook with same layout. We present two variants of the model, one with stacked LSTM layers and one with a bidirectional LSTM layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/blob/master/Chapter_9/language_model_stacked_lstm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laf8nuPYIRQO"
   },
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lqviC-HVpmm2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "GM1hw8f4p67G",
    "outputId": "55998598-2d64-4189-edb7-97fa21edcec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version=2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version={}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZ3sFTVvIhwD"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nr9VOpsop81u"
   },
   "outputs": [],
   "source": [
    "# https://www.gutenberg.org/ebooks/2600\n",
    "datafile_path = r'warpeace_2600-0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "g_6iVpkqqEAN",
    "outputId": "3a0a2a76-0d56-4b63-f9b3-3cebbd0cb784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book contains a total of 2051344 characters\n"
     ]
    }
   ],
   "source": [
    "# Load the text file\n",
    "text = open(datafile_path, 'rb').read().decode(encoding='utf-8')\n",
    "print ('Book contains a total of {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "WUcBctq8qFDq",
    "outputId": "6d0066da-e20d-4898-addc-f087ca27b606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "BOOK ONE: 1805\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "CHAPTER I\r\n",
      "\r\n",
      "“Well, Prince, so Genoa and Lucca are now just family estates of the\r\n",
      "Buonapartes. But I warn you, if you don’t tell me that this means war,\r\n",
      "if you still try to defend the infamies and horrors perpetrated by that\r\n",
      "Antichrist—I really believe he is Antichrist—I will have nothing\r\n",
      "more to do with you and you are no longer my friend, no longer my\r\n",
      "‘faithful slave,’ as you call yourself! But how do you do? I see I\r\n",
      "have frightened you—sit down and tell me al\n"
     ]
    }
   ],
   "source": [
    "idx = 8091\n",
    "print(text[idx:idx+500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NK5VucWVqNiG"
   },
   "outputs": [],
   "source": [
    "# We remove first 8k characters to remove \n",
    "# details related to project gutenberg\n",
    "text = text [8091:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "BgqxMDT1qN1j",
    "outputId": "7f279d68-0c22-4101-c2de-6127bb3dd8b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7K7kUR6Iup6"
   },
   "source": [
    "## Prepare Dataset\n",
    "+ Dictionary of character to index mapping\n",
    "+ Inverse mapping of index to character mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "G-1s-z3RqOG7"
   },
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "14v7MGryqOV2",
    "outputId": "71a1a5df-f323-47ca-b13b-6350cfaeb1d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  '!' :   3,\n",
      "  '(' :   4,\n",
      "  ')' :   5,\n",
      "  '*' :   6,\n",
      "  ',' :   7,\n",
      "  '-' :   8,\n",
      "  '.' :   9,\n",
      "  '0' :  10,\n",
      "  '1' :  11,\n",
      "  '2' :  12,\n",
      "  '3' :  13,\n",
      "  '4' :  14,\n",
      "  '5' :  15,\n",
      "  '6' :  16,\n",
      "  '7' :  17,\n",
      "  '8' :  18,\n",
      "  '9' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6kEpFB1I9NY"
   },
   "source": [
    "### Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Dp_GTAFiqVq1",
    "outputId": "20f3539b-f4a6-4a0c-8f39-5124eb98eefa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n“Well, Prince, so G' ---- char-2-int ----  [ 0 98 45 53 60 60  7  2 38 66 57 62 51 53  7  2 67 63  2 29]\n"
     ]
    }
   ],
   "source": [
    "print ('{} ---- char-2-int ----  {}'.format(repr(text[40:60]), text_as_int[40:60]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Emy0LrJtJFbw"
   },
   "source": [
    "### Prepare Batch of Training Samples\n",
    "+ Sequence length limit to 100\n",
    "+ Use ``tf.data`` API to prepare batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "0XdE0xYcqWJC",
    "outputId": "bb1f6943-e514-4bde-b748-b776cc8cc010"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\n",
      "\n",
      "B\n",
      "O\n",
      "O\n",
      "K\n",
      " \n",
      "O\n",
      "N\n",
      "E\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "ws-2k5p2qWWG",
    "outputId": "8718f678-274e-496b-dcbb-681c3de174ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\r\\nBOOK ONE: 1805\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCHAPTER I\\r\\n\\r\\n“Well, Prince, so Genoa and Lucca are now just family estate'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'s of the\\r\\nBuonapartes. But I warn you, if you don’t tell me that this means war,\\r\\nif you still try to'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "' defend the infamies and horrors perpetrated by that\\r\\nAntichrist—I really believe he is Antichrist—I '\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'will have nothing\\r\\nmore to do with you and you are no longer my friend, no longer my\\r\\n‘faithful slave'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "',’ as you call yourself! But how do you do? I see I\\r\\nhave frightened you—sit down and tell me all the'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "' news.”\\r\\n\\r\\nIt was in July, 1805, and the speaker was the well-known Anna Pávlovna\\r\\nSchérer, maid of h'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'onor and favorite of the Empress Márya Fëdorovna.\\r\\nWith these words she greeted Prince Vasíli Kurágin'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "', a man of high\\r\\nrank and importance, who was the first to arrive at her reception. Anna\\r\\nPávlovna ha'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'d had a cough for some days. She was, as she said, suffering\\r\\nfrom la grippe; grippe being then a new'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "' word in St. Petersburg, used\\r\\nonly by the elite.\\r\\n\\r\\nAll her invitations without exception, written i'\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(10):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "    print(\"-\"*110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfJ8lzfSJuBU"
   },
   "source": [
    "### Prepare Input->Target samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "L-7b_73PqWji"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    \"\"\"\n",
    "    Utility which takes a chunk of input text and target as one position shifted form of input chunk.\n",
    "    Parameters:\n",
    "        chunk: input list of words\n",
    "    Returns:\n",
    "        Tuple-> input_text(i.e. chunk minus last word),target_text(input chunk minus the first word)\n",
    "    \"\"\"\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "ZNpd4pilqWw2",
    "outputId": "4ccace3f-935c-42b0-de0e-4a8bbd6d93f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '\\r\\nBOOK ONE: 1805\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCHAPTER I\\r\\n\\r\\n“Well, Prince, so Genoa and Lucca are now just family estat'\n",
      "Target data: '\\nBOOK ONE: 1805\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCHAPTER I\\r\\n\\r\\n“Well, Prince, so Genoa and Lucca are now just family estate'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MW75dGqMqlAj"
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 128\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "duuMnJEDqlNX",
    "outputId": "d8e73f32-e065-407e-e658-d7a2aebdb5fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape=<BatchDataset shapes: ((128, 100), (128, 100)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(\"Dataset Shape={}\".format(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qL12LqyWJ0tn"
   },
   "source": [
    "## Prepare Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "j_sIzibsqlof"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size,is_bidirectional=False):\n",
    "    \"\"\"\n",
    "    Utility to create a model object.\n",
    "    Parameters:\n",
    "        vocab_size: number of unique characters\n",
    "        embedding_dim: size of embedding vector. This typically in powers of 2, i.e. 64, 128, 256 and so on\n",
    "        rnn_units: number of LSTM units to be used\n",
    "        batch_size: batch size for training the model\n",
    "    Returns:\n",
    "        tf.keras model object\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]))\n",
    "    if is_bidirectional:\n",
    "      model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform')))\n",
    "    else:\n",
    "      model.add(tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'))\n",
    "    model.add(tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'))\n",
    "    model.add(tf.keras.layers.Dense(vocab_size))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipJBSRk_KEGK"
   },
   "source": [
    "### Define the Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6BeShRs8qwCF"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "I-LCpWVEqwU0"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "iujYNi4cqv5N",
    "outputId": "f1273616-7886-41cc-9330-fd6f2128a55d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 256)          25600     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (128, None, 1024)         5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (128, None, 1024)         8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 100)          102500    \n",
      "=================================================================\n",
      "Total params: 13,767,780\n",
      "Trainable params: 13,767,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "IAUK1SDWql6N"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xM5dHsLwrx-S"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAMJXmvYKSGh"
   },
   "source": [
    "### Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4BpfQbQXqmec"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = r'data/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "m6-WTUO9q7is",
    "outputId": "c5ae9d16-edd9-4142-f1ea-bcc8f99646d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "158/158 [==============================] - 30s 188ms/step - loss: 2.6990\n",
      "Epoch 2/25\n",
      "158/158 [==============================] - 30s 188ms/step - loss: 1.7935\n",
      "Epoch 3/25\n",
      "158/158 [==============================] - 30s 188ms/step - loss: 1.4524\n",
      "Epoch 4/25\n",
      "158/158 [==============================] - 30s 188ms/step - loss: 1.3151\n",
      "Epoch 5/25\n",
      "158/158 [==============================] - 30s 188ms/step - loss: 1.2415\n",
      "Epoch 6/25\n",
      "158/158 [==============================] - 30s 187ms/step - loss: 1.1903\n",
      "Epoch 7/25\n",
      "158/158 [==============================] - 30s 188ms/step - loss: 1.1502\n",
      "Epoch 8/25\n",
      "158/158 [==============================] - 30s 187ms/step - loss: 1.1153\n",
      "Epoch 9/25\n",
      "158/158 [==============================] - 30s 188ms/step - loss: 1.0830\n",
      "Epoch 10/25\n",
      "158/158 [==============================] - 30s 188ms/step - loss: 1.0520\n",
      "Epoch 11/25\n",
      "158/158 [==============================] - 30s 188ms/step - loss: 1.0195\n",
      "Epoch 12/25\n",
      "158/158 [==============================] - 30s 189ms/step - loss: 0.9877\n",
      "Epoch 13/25\n",
      "158/158 [==============================] - 29s 187ms/step - loss: 0.9546\n",
      "Epoch 14/25\n",
      "158/158 [==============================] - 30s 188ms/step - loss: 0.9226\n",
      "Epoch 15/25\n",
      "158/158 [==============================] - 30s 187ms/step - loss: 0.8877\n",
      "Epoch 16/25\n",
      "158/158 [==============================] - 30s 189ms/step - loss: 0.8544\n",
      "Epoch 17/25\n",
      "158/158 [==============================] - 30s 188ms/step - loss: 0.8196\n",
      "Epoch 18/25\n",
      "158/158 [==============================] - 30s 188ms/step - loss: 0.7846\n",
      "Epoch 19/25\n",
      "158/158 [==============================] - 30s 189ms/step - loss: 0.7502\n",
      "Epoch 20/25\n",
      "158/158 [==============================] - 30s 187ms/step - loss: 0.7169\n",
      "Epoch 21/25\n",
      "158/158 [==============================] - 30s 188ms/step - loss: 0.6844\n",
      "Epoch 22/25\n",
      "158/158 [==============================] - 30s 187ms/step - loss: 0.6527\n",
      "Epoch 23/25\n",
      "158/158 [==============================] - 30s 188ms/step - loss: 0.6236\n",
      "Epoch 24/25\n",
      "158/158 [==============================] - 30s 189ms/step - loss: 0.5942\n",
      "Epoch 25/25\n",
      "158/158 [==============================] - 30s 188ms/step - loss: 0.5684\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IH2kBWPjKZ_p"
   },
   "source": [
    "## Generate Fake Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5O5OelGKokv"
   },
   "source": [
    "### Load Latest Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "710QCwSJq78O",
    "outputId": "de86a2b0-ee11-4d01-95a8-486a7e032248"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'data/training_checkpoints/ckpt_25'"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch the latest checkpoint from the model directory\n",
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "C33Lo5A4q7yQ"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "VM5cCjyeq7ZP",
    "outputId": "7ac46319-ca4f-4df4-e953-8ed06debf54b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            25600     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (1, None, 1024)           8392704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 100)            102500    \n",
      "=================================================================\n",
      "Total params: 13,767,780\n",
      "Trainable params: 13,767,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_J0tN6RK43x"
   },
   "source": [
    "### Utility Function to Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Zge9K2AIO3TF"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, mode='greedy', context_string='Hello', num_generate=1000, \n",
    "                  temperature=1.0):\n",
    "    \"\"\"\n",
    "    Utility to generate text given a trained model and context\n",
    "    Parameters:\n",
    "        model: tf.keras object trained on a sufficiently sized corpus\n",
    "        mode: decoding mode. Default is greedy. Other mode is\n",
    "              sampling (set temperature)\n",
    "        context_string: input string which acts as context for the model\n",
    "        num_generate: number of characters to be generated\n",
    "        temperature: parameter to control randomness of outputs\n",
    "    Returns:\n",
    "        string : context_string+text_generated\n",
    "    \"\"\"\n",
    "\n",
    "    # vectorizing: convert context string into string indices\n",
    "    input_eval = [char2idx[s] for s in context_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # String for generated characters\n",
    "    text_generated = []\n",
    "    beam_input_predictions = []\n",
    "    model.reset_states()\n",
    "    # Loop till required number of characters are generated\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        if mode == 'greedy':\n",
    "          predicted_id = np.argmax(predictions[0])\n",
    "          \n",
    "        elif mode == 'sampling':\n",
    "          # temperature helps control the character returned by the model.\n",
    "          predictions = predictions / temperature\n",
    "          # Sampling over a categorical distribution\n",
    "          predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # predicted character acts as input for next step\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    return (context_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSviXqdnK-gW"
   },
   "source": [
    "### Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "JP3bh8OorG9c",
    "outputId": "46f65d47-ad4b-476a-f438-dfe6c27ae4ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was in July, 1805” and the general course of which she did not\r\n",
      "und\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, mode= 'greedy', context_string=u\"It was in July, 1805\",num_generate=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbFJp-h8LF6E"
   },
   "source": [
    "### Sampled Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "feWUSIl0dMlu",
    "outputId": "fec6f4d0-f96e-422f-9ce7-7605bf182adc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was in July, 1805, and the general\r\n",
      "who attracted him as a child, her own son, who was still five him a broad and in \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, mode= 'sampling', context_string=u\"It was in July, 1805\",num_generate=100,temperature=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "KeEEU42VcNkH",
    "outputId": "f6f9c03e-d84a-4052-ad3c-6f79ba3b933a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was in July, 1805, and two old\r\n",
      "prince and he and went out into the deserted field of reconciling the trousseau is at\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, mode= 'sampling', context_string=u\"It was in July, 1805\",num_generate=100,temperature=0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "iOI_Yh5-WS9t",
    "outputId": "64fc4f52-3284-478e-a5a3-7159d6d9a52f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was in July, 1805, and heaven only would the\r\n",
      "count never died so easily to do, since Boguchárovo, ready to\r\n",
      "full spe\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, mode= 'sampling', context_string=u\"It was in July, 1805\",num_generate=100,temperature=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDPPRnG-PRAn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "language_model_stacked_lstm.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
